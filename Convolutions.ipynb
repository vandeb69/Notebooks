{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Operations using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Convolutional Operation Mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Mathematically, the discrete convolution operation performed on two functions defined on finite sets of integers is formulated as follows:\n",
    "\n",
    "$$(f * g)[n] = \\sum_{m=-M}^M f[n-m]g[m]$$\n",
    "\n",
    "where $g$ has finite support in the set $\\{-M, -M+1, \\ldots, M-1, M\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's implement this (in an unoptimized form) using Python with functions $f: [0, 1, 2, 3, 4] \\mapsto [1, 2, -1, 1, -3]$, and $g: [-1, 0, 1] \\mapsto [-1, 0, 1]$. In order for the convolution function to work for every position of $f$, we first pad $f$ with zeroes at position -1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from infix import or_infix as infix\n",
    "\n",
    "f = {-1: 0, 0: 1, 1: 2, 2: -1, 3: 1, 4: -3, 5: 0}\n",
    "g = {-1: -1, 0: 0, 1: 1}\n",
    "\n",
    "@infix\n",
    "def cnv(f, g):\n",
    "    def conv_func(n):\n",
    "        M = int(len(g)/2)\n",
    "        res = 0\n",
    "        for m in range(-M, M+1, 1):\n",
    "            res += f[n - m] * g[m]\n",
    "        return res\n",
    "    return conv_func\n",
    "\n",
    "for n in [0, 1, 2, 3, 4]:\n",
    "    print((f |cnv| g)(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So in words, the convolution operator first reverses the order of the elements in $g$ and then calculates the sum of the element-wise product of a $g$-sized window of $f$ and the reversed elements of $g$, at every position in $f$. Note however, that in machine learning applications, one usually doesn't bother first reversing the elements of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Dot Product\n",
    "\n",
    "Summing over element-wise products is rarely being done as in the code above, but is usually implemented more efficiently with dot products. Let's illustrate this with two vectors of the same length, $\\mathbf{a} = [a_1, a_2, \\ldots, a_n]$, and $\\mathbf{b} = [b_1, b_2, \\ldots, b_n]$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}\\mathbf{b}^\\top = \\sum_i^n a_i b_i = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[0, 1, 2]])\n",
    "b = np.array([[1, 0, -1]])\n",
    "print(np.dot(a, b.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions Implemented Efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However with the dot product, we only got rid of one for-loop in our initial implementation of the convolutional operation, as the dot product still needs to be applied on all positions $n$ in the function $f$. Instead of applying the dot products separately for every position, the convolution operation can be vectorized in a single matrix multiplication. Let's first see the simplest situation, with convolutions in one spatial dimension, and with only 1 feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### One Spatial Dimension, One Feature\n",
    "\n",
    "Consider an $n \\times 1$ feature matrix $\\mathbf{X}$ with elements $x_{i}$, where $i \\in \\{1, 2, \\ldots, n\\}$ is not arbitrary but represents a certain ordering or position, and a $k \\times 1$ filter $\\mathbf{W}$ where $k \\leq n$, as in the left part of the figure below (with stride 1). Note in the figure below, that convolutions decrease the length of the feature matrix by $k - 1$, if strides of 1 are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![image.png](http://cs231n.github.io/assets/cnn/stride.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-3]\n",
      " [ 0]]\n",
      "(7, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.transpose(np.array([[0, 1, 2, -1, 1, -3, 0]]))\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 0]\n",
      " [-1]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "W = np.transpose(np.array([[1, 0, -1]]))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we will create a new feature matrix $\\hat{\\mathbf{X}}$ with shape $(n - k + 1) \\times k$ in which the rows are the vector for which we need to perform the dot product with the filter $\\mathbf{W}$. We can do this with fancy indexing like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 1  2 -1]\n",
      " [ 2 -1  1]\n",
      " [-1  1 -3]\n",
      " [ 1 -3  0]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "ix = np.array([[0, 1, 2], \n",
    "               [1, 2, 3], \n",
    "               [2, 3, 4],\n",
    "               [3, 4, 5],\n",
    "               [4, 5, 6]])\n",
    "Xhat = np.squeeze(X[ix])\n",
    "print(Xhat)\n",
    "print(Xhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the matrix multiplication $\\mathbf{Z} = \\hat{\\mathbf{X}}\\mathbf{W}$ yields the convolution as a $(n - k + 1) \\times 1$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]]\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "Z = np.dot(Xhat, W)\n",
    "print(Z)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### One Spatial Dimension, Multiple Features (Channels)\n",
    "\n",
    "The above can be extended to more features, also called channels: now $\\mathbf{X}$ is a $n \\times d$ matrix, where $d$ is the number of features. \n",
    "\n",
    "When there are multiple features, the convolution operation doesn't only aggregate along the spatial dimension, but does so over the feature dimension too. So the filter also needs to have an extra dimension according to the number of features: now $\\mathbf{W}$ is a $k \\times d$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 1  3]\n",
      " [ 2 -2]\n",
      " [-1  2]\n",
      " [ 1  0]\n",
      " [-3  1]\n",
      " [ 0 -1]]\n",
      "(7, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.transpose(np.array([[0, 1, 2, -1, 1, -3, 0],\n",
    "                           [1, 3, -2, 2, 0, 1, -1]]))\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1]\n",
      " [ 0  0]\n",
      " [-1  1]]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "W = np.transpose(np.array([[1, 0, -1],\n",
    "                           [-1, 0, 1]]))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To vectorize the convolution with multiple features, we first need to flatten the filter to a $kd \\times 1$ column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [ 1]]\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "W = W.reshape((6, 1))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With fancy indexing we create a new $(n - k + 1) \\times kd$ feature matrix $\\hat{\\mathbf{X}}$ where the elements in every row correspond with the right weights in the flattened filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  3  2 -2]\n",
      " [ 1  3  2 -2 -1  2]\n",
      " [ 2 -2 -1  2  1  0]\n",
      " [-1  2  1  0 -3  1]\n",
      " [ 1  0 -3  1  0 -1]]\n",
      "(5, 6)\n"
     ]
    }
   ],
   "source": [
    "ps_ix = np.array([[0, 0, 1, 1, 2, 2], \n",
    "                  [1, 1, 2, 2, 3, 3],\n",
    "                  [2, 2, 3, 3, 4, 4],\n",
    "                  [3, 3, 4, 4, 5, 5],\n",
    "                  [4, 4, 5, 5, 6, 6]])\n",
    "ft_ix = np.array([[0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1]])\n",
    "Xhat = X[ps_ix, ft_ix]\n",
    "print(Xhat)\n",
    "print(Xhat.shape)\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the matrix multiplication $\\mathbf{Z} = \\hat{\\mathbf{X}}\\mathbf{W}$ still yields the convolution as a $(n - k + 1) \\times 1$ matrix, as the convolution has aggregated across both spatial and feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xrow' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-af90fc26d5b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xrow' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "out = np.dot(Xhat, W)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multiple Spatial Dimensions, One Feature\n",
    "\n",
    "Let's now consider the situation where the convolution happens in two dimensions instead of one. Now we have an $n_h \\times n_w$ feature matrix $\\mathbf{X}$, and a $k_h \\times k_w$ filter $\\mathbf{W}$. Now on every convolution position, the feature values within a window the same size of the filter are aggregated to one value, and the positions can change in two directions, as illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![image2.png](http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1 -1 -1  2]\n",
      " [-1  1  0 -2  1]\n",
      " [-1  0 -1  0 -1]\n",
      " [-3 -2 -1  1 -1]\n",
      " [-1 -3 -2 -3 -3]]\n",
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(-3, 3, size=(5, 5))\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2  2  0]\n",
      " [ 2  2  2]\n",
      " [-3 -1 -1]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-3, 3, size=(3,3))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To deal with the spatial dimensions, we can again flatten the filter to a $k_h k_w \\times 1$ column matrix. Then later the results will be reshaped back to the original spatial shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [-3]\n",
      " [-1]\n",
      " [-1]]\n",
      "(9, 1)\n"
     ]
    }
   ],
   "source": [
    "W = W.reshape((9, 1))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the fancy indexing of the feature matrix is a bit more complex. The feature matrix will be transformed to a $(n_h - k_h + 1)(n_w - k_w + 1) \\times k_h k_w$ feature matrix $\\hat{\\mathbf{X}}$. The elements in every row correspond with the elements in the flattened filter, and the rows correspond with all possible positions in $h$ and $w$ directions. The $h$ and $w$ indexes need to be constructed in a way so that the original spatial dimensions can be easily obtained by reshaping afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1 -1 -1  1  0 -1  0 -1]\n",
      " [ 1 -1 -1  1  0 -2  0 -1  0]\n",
      " [-1 -1  2  0 -2  1 -1  0 -1]\n",
      " [-1  1  0 -1  0 -1 -3 -2 -1]\n",
      " [ 1  0 -2  0 -1  0 -2 -1  1]\n",
      " [ 0 -2  1 -1  0 -1 -1  1 -1]\n",
      " [-1  0 -1 -3 -2 -1 -1 -3 -2]\n",
      " [ 0 -1  0 -2 -1  1 -3 -2 -3]\n",
      " [-1  0 -1 -1  1 -1 -2 -3 -3]]\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "h_ix = np.array([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "                 [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "                 [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "                 [1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "                 [1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "                 [1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "                 [2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "                 [2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "                 [2, 2, 2, 3, 3, 3, 4, 4, 4]])\n",
    "w_ix = np.array([[0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
    "                 [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "                 [2, 3, 4, 2, 3, 4, 2, 3, 4],\n",
    "                 [0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
    "                 [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "                 [2, 3, 4, 2, 3, 4, 2, 3, 4],\n",
    "                 [0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
    "                 [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "                 [2, 3, 4, 2, 3, 4, 2, 3, 4]])\n",
    "Xhat = X[h_ix, w_ix]\n",
    "print(Xhat)\n",
    "print(Xhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the dot product will yield a $(n_h - k_h + 1)(n_w - k_w + 1) \\times 1$ column matrix, which should be reshaped to a $(n_h - k_h + 1) \\times (n_w - k_w + 1)$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2]\n",
      " [-5]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 2]\n",
      " [-5]\n",
      " [-2]\n",
      " [ 8]\n",
      " [12]]\n",
      "[[ 2 -5  2]\n",
      " [12  2 -5]\n",
      " [-2  8 12]]\n"
     ]
    }
   ],
   "source": [
    "out = np.dot(Xhat, W)\n",
    "print(out)\n",
    "print(out.reshape((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multiple Spatial Dimensions, Multiple Features (Channels)\n",
    "\n",
    "We can again extend this to multiple features. Now we have a $n_h \\times n_w \\times d$ feature matrix and a $k_h \\times k_w \\times d$ filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 -3 -1 -2 -2]\n",
      "  [-2  1  0 -1  0]\n",
      "  [-2  1  0 -3 -2]\n",
      "  [-2  2 -2 -3  2]\n",
      "  [ 2 -2  0 -3  1]]\n",
      "\n",
      " [[-3  0  2 -2 -2]\n",
      "  [-2  0 -3 -1  0]\n",
      "  [-1 -3 -2  1 -3]\n",
      "  [ 1  1 -2  2 -1]\n",
      "  [ 0  2 -2  1 -2]]]\n",
      "(5, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(-3, 3, (5, 5, 2))\n",
    "print(X.transpose((2, 0, 1))) # transposed for display\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-3 -1 -1]\n",
      "  [ 1  1  0]\n",
      "  [-2  0  1]]\n",
      "\n",
      " [[ 1 -3  0]\n",
      "  [ 2  1 -1]\n",
      "  [ 2  0  0]]]\n",
      "(3, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-3, 3, (3, 3, 2))\n",
    "print(W.transpose((2, 0, 1))) # transposed for display\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The filter needs to be flattened to a $k_h k_w d \\times 1$ column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-3]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]]\n",
      "(18, 1)\n"
     ]
    }
   ],
   "source": [
    "W = W.reshape((18, 1))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use fancy indexing on the feature matrix to a transformed it to a $(n_h - k_h + 1)(n_w - k_w + 1) \\times k_h k_w d$ feature matrix, where elements in every row correspond with the elements in the weight column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -3 -3  0 -1  2 -2 -2  1  0  0 -3 -2 -1  1 -3  0 -2]\n",
      " [-3  0 -1  2 -2 -2  1  0  0 -3 -1 -1  1 -3  0 -2 -3  1]\n",
      " [-1  2 -2 -2 -2 -2  0 -3 -1 -1  0  0  0 -2 -3  1 -2 -3]\n",
      " [-2 -2  1  0  0 -3 -2 -1  1 -3  0 -2 -2  1  2  1 -2 -2]\n",
      " [ 1  0  0 -3 -1 -1  1 -3  0 -2 -3  1  2  1 -2 -2 -3  2]\n",
      " [ 0 -3 -1 -1  0  0  0 -2 -3  1 -2 -3 -2 -2 -3  2  2 -1]\n",
      " [-2 -1  1 -3  0 -2 -2  1  2  1 -2 -2  2  0 -2  2  0 -2]\n",
      " [ 1 -3  0 -2 -3  1  2  1 -2 -2 -3  2 -2  2  0 -2 -3  1]\n",
      " [ 0 -2 -3  1 -2 -3 -2 -2 -3  2  2 -1  0 -2 -3  1  1 -2]]\n",
      "(9, 18)\n"
     ]
    }
   ],
   "source": [
    "h_ix = np.array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], \n",
    "                 [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], \n",
    "                 [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], \n",
    "                 [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3], \n",
    "                 [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3], \n",
    "                 [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3], \n",
    "                 [2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4], \n",
    "                 [2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4],\n",
    "                 [2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]])\n",
    "w_ix = np.array([[0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2], \n",
    "                 [1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3], \n",
    "                 [2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4],\n",
    "                 [0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2], \n",
    "                 [1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3], \n",
    "                 [2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4],\n",
    "                 [0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2], \n",
    "                 [1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3], \n",
    "                 [2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4]])\n",
    "ft_ix = np.array([[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]])\n",
    "Xhat = X[h_ix, w_ix, ft_ix]\n",
    "print(Xhat)\n",
    "print(Xhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have a $(n_h - k_h + 1)(n_w - k_w + 1) \\times k_h k_w d$ feature matrix, and a $k_h k_w d \\times 1$ filter. The dot product yields a $(n_h - k_h + 1) (n_w - k_w + 1) \\times 1$ column matrix, which again should be reshaped to a $(n_h - k_h + 1) \\times (n_w - k_w + 1)$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2]\n",
      " [-6]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [-6]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 6]\n",
      " [-9]]\n",
      "[[-2 -6  1]\n",
      " [ 3 -6  0]\n",
      " [14  6 -9]]\n"
     ]
    }
   ],
   "source": [
    "out = np.dot(Xhat, W)\n",
    "print(out)\n",
    "print(out.reshape((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multiple filters\n",
    "\n",
    "In the context of neural networks, usually multiple filters are applied in hidden layers, resulting in similar results as above, except that there will be an extra dimension corresponding with the number of filters, which after passing through an activation function will serve as input features for the next neural network layer. In this case the filter matrix is $k_h \\times k_w \\times d) \\times o$, where $o$ is the number of filters. This is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![image3.gif](https://cdn-images-1.medium.com/max/1600/1*_34EtrgYk6cQxlJ2br51HQ.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1]\n",
      " [-3  1]\n",
      " [ 0  2]\n",
      " [ 2  1]\n",
      " [-1  2]\n",
      " [-1 -2]\n",
      " [ 2  1]\n",
      " [-1  0]\n",
      " [-3 -1]\n",
      " [ 1  0]\n",
      " [ 2 -1]\n",
      " [-2 -2]\n",
      " [-3  2]\n",
      " [-3 -2]\n",
      " [-1 -2]\n",
      " [-2 -3]\n",
      " [-2 -2]\n",
      " [ 2 -2]]\n",
      "(18, 2)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-3, 3, (3*3*2, 2))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18  -4]\n",
      " [ 28  25]\n",
      " [  5  15]\n",
      " [  5   4]\n",
      " [ -3  12]\n",
      " [ 26   3]\n",
      " [-21  12]\n",
      " [ 15  -9]\n",
      " [ 29   5]]\n",
      "(9, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 18,  28,   5],\n",
       "        [  5,  -3,  26],\n",
       "        [-21,  15,  29]],\n",
       "\n",
       "       [[ -4,  25,  15],\n",
       "        [  4,  12,   3],\n",
       "        [ 12,  -9,   5]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.dot(Xhat, W)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "out = out.reshape((3, 3, 2))\n",
    "np.transpose(out, (2, 0, 1)) # transposed for display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
